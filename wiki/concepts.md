###1 

academia strength is knowing what counterexamples are relevant and which aren't

[Tarleton Gillespie - politics of 'platforms'](http://nms.sagepub.com/content/12/3/347.abstract) - understanding of how term 'platforms' was used to political ends

[ronald kline - information technology as a keyword](http://muse.jhu.edu/journals/technology_and_culture/v047/47.3kline.html) - analysis-free history of the term

[leo marx - the emergence of a hazardous concept](http://muse.jhu.edu/journals/technology_and_culture/v051/51.3.marx.html) - 'technology' not popular term until 1900-1920. word came to fill ' a conceptual—void, which is to say, an awareness of certain novel developments in society and culture for which no adequate name had yet become available'. **throreau quote** == kevin kelly

## 2

>Martin Hilbert argues[..] to quantify the amount of information handled by society (Hilbert, 2012). [..] quantify *the amount of information you handle (i.e., create or store or consume etc.) in the course of one four-to-eight hour period,* whether at home, at school, or elsewhere. Remember to include not just the *types of sources discussed by Bohn & Short and Lyman & Varian, but also the “incidental” and ambient information that we encounter as we drive to work, eat breakfast, or call home*; [..] Assess the statistical feasibility and analytic insightfulness of the result: what questions does the exercise raise for a theory of “information” in the large?to the special section of IJOC [see "background reading," below].

[Roger Bohn & James Short - how much information?](http://hmi.ucsd.edu/pdf/HMI_2009_ConsumerReport_Dec9_2009.pdf) - measured by amount of data deliered to people .. well, how much of that are they actually processing? our cursory measurements indcate that the amount of information delivered by modern media formats might outstrip our cognitive processing abilities.....

[peter lyman, hal varian etc - how much information?](http://www2.sims.berkeley.edu/research/projects/how-much-info-2003/execsum.htm) - figure of 800MB per person per year made me lol .. saw figure of 1.8 million megabytes in mit tech review article 2013

[hilbert - how much information?](http://ijoc.org/index.php/ijoc/article/view/1318/746) - history of this question since aristotle's student demitrious consutling on library of alexandria

## 3

[claude shannon - mathematical theory of communication (1948)](http://dl.acm.org/citation.cfm?id=584093)

actual message is one message selected from a se of possible messages.

prediction: logarithms will appear and reappear

flip/flop circuts motivate bits

wanst obvious in 1948 that fig 1 would be so generalizeable.

discrete: finite symbols; 1.1 we see "bits per second"; 1.2 we start to motivate entropy by showing letters in english are not completely random (e.g. 'e' appears more freq than 'q', 'th' more freq than 'xp', etc). so we can model producers of discrete information (i.e. coders, language-users) as generators of stochastic processes, specifically markov models, specifically **"ergodic"** ones (i.e. statistical conditions are teh same in the generation of each sequence - we assume language is (basically) ergodic)

**1.3 - basically horse ebooks** - generating n-grams of english

versus continuous: e.g. ECG

II.XI - noise; signal does nota lways undergo same change in transmission ,, idea of error correction - related to [nyquist-shannon sampling](http://en.wikipedia.org/wiki/Nyquist%E2%80%93Shannon_sampling_theorem) - 'The sampling theorem introduces the concept of a sample-rate that is sufficient for perfect fidelity for the class of bandlimited functions'